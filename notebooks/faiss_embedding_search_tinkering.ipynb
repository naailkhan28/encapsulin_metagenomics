{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching MGnify Protein Database Embeddings Using Faiss\n",
    "\n",
    "## Background\n",
    "\n",
    "So far, the encapsulin hits in this repo have come from two different search methods - either structure search against the ESM Atlas, or functional annotation search against Pfam labels in the MGnify Protein Database.\n",
    "\n",
    "However, we know that protein language models generate meaningful vector representations of protein sequences - so why can't we use vector-based methods to search encapsulin query vectors against a target database of vectors generated for all MGnify proteins?\n",
    "\n",
    "Meta AI have made the embeddings for all MGnify proteins available publicly for download, so we don't have to generate 6B+ vectors ourselves. We can just use ESM-2 to generate embedding vectors for a handful of encapsulin sequences and then search the database using [faiss](https://github.com/facebookresearch/faiss), a package from Facebook AI Research designed to make vector search faster using GPU acceleration.\n",
    "\n",
    "This notebook will experiment with loading, analyzing, and searching the database, and generating the query vectors.\n",
    "\n",
    "## Database Vectors\n",
    "\n",
    "The MGnify Protein Database embeddings can be downloaded using URLs available from the [ESM repo](https://github.com/facebookresearch/esm). I've already downloaded them here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tm_.40_.50_plddt_.40_.50_00.npz\n",
      "tm_.40_.50_plddt_.40_.50_01.npz\n",
      "tm_.40_.50_plddt_.50_.60_00.npz\n",
      "tm_.40_.50_plddt_.50_.60_01.npz\n",
      "tm_.40_.50_plddt_.60_.70_00.npz\n",
      "tm_.40_.50_plddt_.60_.70_01.npz\n",
      "tm_.40_.50_plddt_.70_.80_00.npz\n",
      "tm_.40_.50_plddt_0_.40_00.npz\n",
      "tm_.40_.50_plddt_0_.40_01.npz\n",
      "tm_.50_.60_plddt_.40_.50_00.npz\n"
     ]
    }
   ],
   "source": [
    "!ls ../../sequence_DBs/atlas/embeddings | head -10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load one of these and see what's in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<numpy.lib.npyio.NpzFile at 0x7f278c4bae30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "db = np.load(\"../../sequence_DBs/atlas/embeddings/tm_.40_.50_plddt_.40_.50_00.npz\")\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object is a Numpy NPZ file, a compressed format for storing matrices and vectors. Let's check out what's inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/scratch/slurm_tmpdir/379122/tm_.40_.50_plddt_.40_.50_00/468/MGYP000160952468', array([ 0.05182 , -0.05518 , -0.009895, ..., -0.01677 , -0.0749  ,\n",
      "       -0.012024], dtype=float16))\n"
     ]
    }
   ],
   "source": [
    "for item in db.items():\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate through the NPZ file using the `.items()` method just like a standard Python dictionary - looks like the keys are a string containing the filename, and MGYP protein ID from MGnify, with the value being a Numpy array. Let's check out that array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: MGYP000160952468\n",
      "Shape: (2560,)\n"
     ]
    }
   ],
   "source": [
    "for key, value in db.items():\n",
    "    print(f\"ID: {key.split('/')[-1]}\")\n",
    "    print(f\"Shape: {value.shape}\")\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm curious about something - how many vectors do we have in total? Let's iterate through all the files and find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking file 1\n",
      "Checking file 2\n",
      "Checking file 3\n",
      "Checking file 4\n",
      "Checking file 5\n",
      "Checking file 6\n",
      "Checking file 7\n",
      "Checking file 8\n",
      "Checking file 9\n",
      "Checking file 10\n",
      "Checking file 11\n",
      "Checking file 12\n",
      "Checking file 13\n",
      "Checking file 14\n",
      "Checking file 15\n",
      "Checking file 16\n",
      "Checking file 17\n",
      "Checking file 18\n",
      "Checking file 19\n",
      "Checking file 20\n",
      "Checking file 21\n",
      "Checking file 22\n",
      "Checking file 23\n",
      "Checking file 24\n",
      "Checking file 25\n",
      "Checking file 26\n",
      "Checking file 27\n",
      "Checking file 28\n",
      "Checking file 29\n",
      "Checking file 30\n",
      "Checking file 31\n",
      "Checking file 32\n",
      "Checking file 33\n",
      "Checking file 34\n",
      "Checking file 35\n",
      "Checking file 36\n",
      "Checking file 37\n",
      "Checking file 38\n",
      "Checking file 39\n",
      "Checking file 40\n",
      "Checking file 41\n",
      "Checking file 42\n",
      "Checking file 43\n",
      "Checking file 44\n",
      "Checking file 45\n",
      "Checking file 46\n",
      "Checking file 47\n",
      "Checking file 48\n",
      "Checking file 49\n",
      "Checking file 50\n",
      "Checking file 51\n",
      "Checking file 52\n",
      "Checking file 53\n",
      "Checking file 54\n",
      "Checking file 55\n",
      "Checking file 56\n",
      "Checking file 57\n",
      "Checking file 58\n",
      "Checking file 59\n",
      "Checking file 60\n",
      "Checking file 61\n",
      "Checking file 62\n",
      "Checking file 63\n",
      "Checking file 64\n",
      "Checking file 65\n",
      "Checking file 66\n",
      "Checking file 67\n",
      "Checking file 68\n",
      "Checking file 69\n",
      "Checking file 70\n",
      "Checking file 71\n",
      "Checking file 72\n",
      "Checking file 73\n",
      "Checking file 74\n",
      "Checking file 75\n",
      "Checking file 76\n",
      "Checking file 77\n",
      "Checking file 78\n",
      "Checking file 79\n",
      "Checking file 80\n",
      "Checking file 81\n",
      "Checking file 82\n",
      "Checking file 83\n",
      "Checking file 84\n",
      "Checking file 85\n",
      "Checking file 86\n",
      "Checking file 87\n",
      "Checking file 88\n",
      "Checking file 89\n",
      "Checking file 90\n",
      "Checking file 91\n",
      "Checking file 92\n",
      "Checking file 93\n",
      "Checking file 94\n",
      "Checking file 95\n",
      "Checking file 96\n",
      "Checking file 97\n",
      "Checking file 98\n",
      "Checking file 99\n",
      "Checking file 100\n",
      "Checking file 101\n",
      "Checking file 102\n",
      "Checking file 103\n",
      "Checking file 104\n",
      "Checking file 105\n",
      "Checking file 106\n",
      "Checking file 107\n",
      "Checking file 108\n",
      "Checking file 109\n",
      "Checking file 110\n",
      "Checking file 111\n",
      "Checking file 112\n",
      "Checking file 113\n",
      "Checking file 114\n",
      "Checking file 115\n",
      "Checking file 116\n",
      "Checking file 117\n",
      "Checking file 118\n",
      "Checking file 119\n",
      "Checking file 120\n",
      "Checking file 121\n",
      "Checking file 122\n",
      "Checking file 123\n",
      "Checking file 124\n",
      "Checking file 125\n",
      "Checking file 126\n",
      "Checking file 127\n",
      "Checking file 128\n",
      "Checking file 129\n",
      "Checking file 130\n",
      "Checking file 131\n",
      "Checking file 132\n",
      "Checking file 133\n",
      "Checking file 134\n",
      "Checking file 135\n",
      "Checking file 136\n",
      "Checking file 137\n",
      "Checking file 138\n",
      "Checking file 139\n",
      "Checking file 140\n",
      "Checking file 141\n",
      "Checking file 142\n",
      "Checking file 143\n",
      "Checking file 144\n",
      "Checking file 145\n",
      "Checking file 146\n",
      "Checking file 147\n",
      "Checking file 148\n",
      "Checking file 149\n",
      "49088893\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "length = 0\n",
    "\n",
    "for i, filepath in enumerate(os.listdir(\"../../sequence_DBs/atlas/embeddings/\")):\n",
    "    print(f\"Checking file {i+1}\")\n",
    "    db = np.load(f\"../../sequence_DBs/atlas/embeddings/{filepath}\")\n",
    "    length += len(db.items())\n",
    "\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage in gigabytes: 251.33513216\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory usage in gigabytes: {(length * 2560 * 2) / 1000000000}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "49,088,893 vectors - at 2560 dimensions, and float16 dtype, that's 250 GB of vectors to load into memory! Clearly we can't just brute force everything. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing `faiss` Indexes\n",
    "\n",
    "The way `faiss` works is that we create an `Index` and add all of our vectors to it. We can then do *k*-nearest neighbours search on this index with a set of query vectors.\n",
    "\n",
    "There are lots of different index types and ways of doing this, the documentation is rather cryptic about this and gets into lots of technical jargon around clustering, transforms, search, and other details. However, there is a somewhat helpful [set of guidelines](https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index) around choosing the Index to use.\n",
    "\n",
    "The way I see it is this: the most basic, brute-force search method `IndexFlatL2` is the most accurate, since it does an exhaustive search against all vectors, and stores all vectors uncompressed. As such this is the \"best\" performing method but will take too long and use too much memory for our purposes.\n",
    "\n",
    "There are lots of different ways of saving time and memory, however I've decided to test the approach [outlined here](https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index#if-quite-important-then-opqm_dpqmx4fsr) in the docs. We'd eventually like to run these searches on GPU so our memory limit is around 24 GB (but I'd like some headroom on this figure).\n",
    "\n",
    "Let's test this index with a subset of our data and see what happens. First, we need to load our embedding vectors into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array([db[file] for file in db.files])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esmfold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
